{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the folder containing your text files\n",
        "folder_path = '/content/drive/MyDrive/text_files/'\n",
        "\n",
        "# Print the list of files in the folder\n",
        "print(\"Files in the 'text_files' folder only 5 for sample:\")\n",
        "count=0\n",
        "for file_name in os.listdir(folder_path):\n",
        "    print(file_name)\n",
        "    count+=1\n",
        "    if count>5:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "a727bEki1ZKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPROCESSING"
      ],
      "metadata": {
        "id": "J5bGxA9BvtbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define function for preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [word for word in tokens if word.strip()]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Path to the folder containing your text files\n",
        "folder_path = '/content/drive/MyDrive/text_files/'\n",
        "\n",
        "# Preprocess each text file\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    processed_tokens = preprocess_text(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H46mFqcdwJmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Define function for preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [word for word in tokens if word.strip()]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Path to the folder containing your text files\n",
        "folder_path = '/content/drive/MyDrive/text_files/'\n",
        "\n",
        "# Select 5 sample files\n",
        "sample_files = os.listdir(folder_path)[:5]\n",
        "\n",
        "# Process each sample file\n",
        "for file_name in sample_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # Read the original text\n",
        "    with open(file_path, 'r') as file:\n",
        "        original_text = file.read()\n",
        "\n",
        "    # Print original text\n",
        "    print(\"Original text of file '{}':\".format(file_name))\n",
        "    print(original_text[:500])  # Print first 500 characters\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Lowercase the text\n",
        "    lowercased_text = original_text.lower()\n",
        "\n",
        "    # Print lowercase text\n",
        "    print(\"Lowercased text of file '{}':\".format(file_name))\n",
        "    print(lowercased_text[:500])  # Print first 500 characters\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(lowercased_text)\n",
        "\n",
        "    # Print tokens\n",
        "    print(\"Tokenized text of file '{}':\".format(file_name))\n",
        "    print(tokens[:50])  # Print first 50 tokens\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Print tokens after removing stopwords\n",
        "    print(\"Text of file '{}' after removing stopwords:\".format(file_name))\n",
        "    print(filtered_tokens[:50])  # Print first 50 tokens\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Remove punctuations\n",
        "    filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
        "\n",
        "    # Print tokens after removing punctuations\n",
        "    print(\"Text of file '{}' after removing punctuations:\".format(file_name))\n",
        "    print(filtered_tokens[:50])  # Print first 50 tokens\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    filtered_tokens = [word for word in filtered_tokens if word.strip()]\n",
        "\n",
        "    # Print tokens after removing blank space tokens\n",
        "    print(\"Text of file '{}' after removing blank space tokens:\".format(file_name))\n",
        "    print(filtered_tokens[:50])  # Print first 50 tokens\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Join tokens back into text\n",
        "    processed_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    # Print processed text\n",
        "    print(\"Processed text of file '{}':\".format(file_name))\n",
        "    print(processed_text[:500])  # Print first 500 characters\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # Save preprocessed text to a new file\n",
        "    preprocessed_file_path = os.path.join(folder_path, 'preprocessed_' + file_name)\n",
        "    with open(preprocessed_file_path, 'w') as file:\n",
        "        file.write(processed_text)\n",
        "\n",
        "    print(\"Preprocessed file saved as '{}'\".format('preprocessed_' + file_name))\n",
        "    print(\"----------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "xoJij34EwkFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question2\n"
      ],
      "metadata": {
        "id": "_m4qxq8cx1VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "    tokens = [word for word in tokens if word.strip()]\n",
        "    return tokens\n",
        "\n",
        "# Function to create inverted index\n",
        "def create_inverted_index(folder_path):\n",
        "    inverted_index = {}\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "        tokens = preprocess_text(text)\n",
        "        for position, token in enumerate(tokens):\n",
        "            if token not in inverted_index:\n",
        "                inverted_index[token] = {}\n",
        "            if file_name not in inverted_index[token]:\n",
        "                inverted_index[token][file_name] = set()\n",
        "            inverted_index[token][file_name].add(position)\n",
        "    return inverted_index\n",
        "\n",
        "# Function to save inverted index using pickle\n",
        "def save_inverted_index(inverted_index, file_path):\n",
        "    with open(file_path, 'wb') as file:\n",
        "        pickle.dump(inverted_index, file)\n",
        "\n",
        "# Function to load inverted index using pickle\n",
        "def load_inverted_index(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        inverted_index = pickle.load(file)\n",
        "    return inverted_index\n",
        "\n",
        "# Function to process AND operation\n",
        "def query_AND(terms, inverted_index):\n",
        "    result = set()\n",
        "    for term in terms:\n",
        "        if not result:\n",
        "            result = set(inverted_index.get(term, {}).keys())\n",
        "        else:\n",
        "            result &= set(inverted_index.get(term, {}).keys())\n",
        "    return result\n",
        "\n",
        "# Function to process OR operation\n",
        "def query_OR(terms, inverted_index):\n",
        "    result = set()\n",
        "    for term in terms:\n",
        "        result |= set(inverted_index.get(term, {}).keys())\n",
        "    return result\n",
        "\n",
        "# Function to process AND NOT operation\n",
        "def query_AND_NOT(term1, term2, inverted_index):\n",
        "    result1 = set(inverted_index.get(term1, {}).keys())\n",
        "    result2 = set(inverted_index.get(term2, {}).keys())\n",
        "    return result1 - result2\n",
        "\n",
        "# Function to process OR NOT operation\n",
        "def query_OR_NOT(term1, term2, inverted_index):\n",
        "    result1 = set(inverted_index.get(term1, {}).keys())\n",
        "    result2 = set(inverted_index.get(term2, {}).keys())\n",
        "    return result1 | (set(inverted_index.keys()) - result2)\n",
        "\n",
        "# Function to process queries\n",
        "def process_queries(inverted_index, queries):\n",
        "    results = []\n",
        "    for i in range(0, len(queries), 2):\n",
        "        query_sequence = queries[i].split()\n",
        "        operations = queries[i + 1].split(',')\n",
        "\n",
        "        # Initialize result with all documents\n",
        "        result = set(inverted_index.keys())\n",
        "\n",
        "        # Iterate over each term and operation\n",
        "        for term, op in zip(query_sequence, operations):\n",
        "            if 'AND' in op:\n",
        "                result = query_AND([term], inverted_index)\n",
        "            elif 'OR' in op:\n",
        "                result = query_OR([term], inverted_index)\n",
        "            elif 'NOT' in op:\n",
        "                not_term = op.split()[1]\n",
        "                result = query_AND_NOT(term, not_term, inverted_index)\n",
        "            elif 'OR NOT' in op:\n",
        "                not_term = op.split()[2]\n",
        "                result = query_OR_NOT(term, not_term, inverted_index)\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "# Function to get user input\n",
        "def get_user_input():\n",
        "    num_queries = int(input(\"Enter the number of queries to execute: \"))\n",
        "    queries = []\n",
        "    for _ in range(num_queries):\n",
        "        query_sequence = input(\"Enter query sequence: \")\n",
        "        operations = input(\"Enter operations separated by comma: \")\n",
        "        queries.extend([query_sequence, operations])\n",
        "    return num_queries, queries\n",
        "\n",
        "# Function to display results\n",
        "def display_results(results):\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"Query {i}:\")\n",
        "        print(f\"Number of documents retrieved: {len(result)}\")\n",
        "        print(f\"Name of the documents retrieved: {list(result)}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Example usage\n",
        "    folder_path = '/content/drive/MyDrive/text_files/'\n",
        "    inverted_index = create_inverted_index(folder_path)\n",
        "    save_inverted_index(inverted_index, 'inverted_index.pickle')\n",
        "\n",
        "    loaded_inverted_index = load_inverted_index('inverted_index.pickle')\n",
        "\n",
        "    # Get user input\n",
        "    num_queries, queries = get_user_input()\n",
        "\n",
        "    # Process queries\n",
        "    results = process_queries(loaded_inverted_index, queries)\n",
        "\n",
        "    # Display results\n",
        "    display_results(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "yEbLgn1t0UZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION3\n"
      ],
      "metadata": {
        "id": "m3gK4lBD6ILJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Define function for preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [word for word in tokens if word.strip()]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to create positional index\n",
        "def create_positional_index(folder_path):\n",
        "    positional_index = {}\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        tokens = preprocess_text(text)\n",
        "\n",
        "        for position, token in enumerate(tokens):\n",
        "            if token not in positional_index:\n",
        "                positional_index[token] = {}\n",
        "            if file_name not in positional_index[token]:\n",
        "                positional_index[token][file_name] = []\n",
        "            positional_index[token][file_name].append(position)\n",
        "\n",
        "    return positional_index\n",
        "\n",
        "# Function to save positional index using pickle\n",
        "def save_positional_index(positional_index, file_path):\n",
        "    with open(file_path, 'wb') as file:\n",
        "        pickle.dump(positional_index, file)\n",
        "\n",
        "# Function to load positional index using pickle\n",
        "def load_positional_index(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        positional_index = pickle.load(file)\n",
        "    return positional_index\n",
        "\n",
        "# Function to process queries using positional index\n",
        "def process_queries(positional_index, queries):\n",
        "    results = []\n",
        "    for query in queries:\n",
        "        query_terms = preprocess_text(query)\n",
        "        retrieved_docs = None\n",
        "        for term in query_terms:\n",
        "            if term in positional_index:\n",
        "                if retrieved_docs is None:\n",
        "                    retrieved_docs = set(positional_index[term].keys())\n",
        "                else:\n",
        "                    retrieved_docs &= set(positional_index[term].keys())\n",
        "        results.append(retrieved_docs)\n",
        "    return results\n",
        "\n",
        "# Function to get user input\n",
        "def get_user_input():\n",
        "    num_queries = int(input(\"Enter the number of queries to execute: \"))\n",
        "    queries = []\n",
        "    for _ in range(num_queries):\n",
        "        query = input(\"Enter phrase query: \")\n",
        "        queries.append(query)\n",
        "    return queries\n",
        "\n",
        "# Function to display results\n",
        "def display_results(results):\n",
        "    for i, docs in enumerate(results, 1):\n",
        "       print(f\"Number of documents retrieved for query {i} using positional index: {len(docs)}\")\n",
        "       print(f\"Names of documents retrieved for query {i} using positional index: {', '.join(docs)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to the folder containing your text files\n",
        "    folder_path = '/content/drive/MyDrive/text_files/'\n",
        "\n",
        "    # Create positional index\n",
        "    positional_index = create_positional_index(folder_path)\n",
        "\n",
        "    # Save positional index\n",
        "    save_positional_index(positional_index, 'positional_index.pickle')\n",
        "\n",
        "    # Load positional index\n",
        "    loaded_positional_index = load_positional_index('positional_index.pickle')\n",
        "\n",
        "    # Get user input\n",
        "    queries = get_user_input()\n",
        "\n",
        "    # Process queries\n",
        "    results = process_queries(loaded_positional_index, queries)\n",
        "\n",
        "    # Display results\n",
        "    display_results(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HogOCqev6KzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VmAazNlv_V9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}